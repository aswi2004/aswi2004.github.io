layout: single
title: "로지스틱 회귀 모델 구현"
use_math: true
comments: true


## 조기 종료를 이용한 배치 경사하강법으로 로지스틱 회귀 구현.  <br/>(단, 사이킷런을 사용하지 않음)

우선 필요한 모듈을 임포트한다.


```python
# 파이썬 ≥3.5 필수
import sys
assert sys.version_info >= (3, 5)

# 사이킷런 ≥0.20 필수
import sklearn
assert sklearn.__version__ >= "0.20"

# 공통 모듈 임포트
import numpy as np
import os

# 깔끔한 그래프 출력을 위해
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# 그림을 저장할 위치
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training_linear_models"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("그림 저장:", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)
    
# 어레이 데이터를 csv 파일로 저장하기
def save_data(fileName, arrayName, header=''):
    np.savetxt(fileName, arrayName, delimiter=',', header=header, comments='')
```

**데이터 준비**  
붓꽃 데이터셋의 꽃잎 길이(petal length)와 꽃잎 너비(petal width) 특성을 이용하여  
꽃의 품종이 버지니카인 것과 아닌 것들로 분류한다.

사이킷런에서 제공하는 붓꽃 데이터는 `Bunch` 자료형이다.  
기본적으로 `dict`자료형과 동일하나,  
차이점은 인덱싱을 속성처럼 처리할 수 있다는 것이다.


```python
# 붓꽃 데이터만 불러오기 위해 sklearn 사용
from sklearn import datasets
iris = datasets.load_iris()
```

타겟은 꽃의 품종이다.  
0: 세토사  
1: 버시컬러  
2: 버지니카


```python
iris.target
```




    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])




```python
X = iris["data"][:, (2,3)]  # 꽃잎 길이와 너비
y = (iris["target"] == 2).astype(np.int)
```

0번 특성에 편향을 추가한다.


```python
X_with_bias = np.c_[np.ones([len(X), 1]), X]
```

반복해도 같은 결과가 나오게 하기 위해 랜덤 시드를 지정한다.`


```python
np.random.seed(2042)
```

**데이터셋 분할**  
데이터셋을 아래처럼 나눈다.
* 훈련 세트: 60%
* 검증 세트: 20%
* 테스트 세트: 20%


```python
test_ratio = 0.2                                         # 테스트 세트 비율 = 20%
validation_ratio = 0.2                                   # 검증 세트 비율 = 20%
total_size = len(X_with_bias)                            # 전체 데이터셋 크기

test_size = int(total_size * test_ratio)                 # 테스트 세트 크기: 전체의 20%
validation_size = int(total_size * validation_ratio)     # 검증 세트 크기: 전체의 20%
train_size = total_size - test_size - validation_size    # 훈련 세트 크기: 전체의 60%
```

`np.random.permutation()` 함수를 이용하여 인덱스를 무작위로 섞는다.


```python
rnd_indices = np.random.permutation(total_size)
```

훈련, 검증, 테스트세트로 분할된 데이터셋의 인덱스를 섞는다.


```python
X_train = X_with_bias[rnd_indices[:train_size]]
y_train = y[rnd_indices[:train_size]]

X_valid = X_with_bias[rnd_indices[train_size:-test_size]]
y_valid = y[rnd_indices[train_size:-test_size]]

X_test = X_with_bias[rnd_indices[-test_size:]]
y_test = y[rnd_indices[-test_size:]]
```


```python
y_train[:5]
```




    array([0, 0, 1, 0, 0])



**로지스틱 모델 구현**

로지스틱 함수는 시그모이드 함수를 사용한다.

$$
\sigma(t) = \frac{1}{1 + e^{-t}}
$$


```python
def logistic(logits):
    return 1.0 / (1 + np.exp(-logits))
```

가중치를 조정해나가기 위해 세타를 생성한다.

$$
\begin{align*}
\hat y^{(i)} & = \theta^{T}\, \mathbf{x}^{(i)} \\
 & = \theta_0 + \theta_1\, \mathbf{x}_1^{(i)} + \cdots + \theta_n\, \mathbf{x}_n^{(i)}
\end{align*}
$$

여기서 특성이 2개(꽃잎 길이, 너비)이므로 n = 2


```python
n_inputs = X_train.shape[1] #편향과 특성의 갯수
Theta = np.random.randn(n_inputs) #편향과 특성의 갯수만큼 세타값 랜덤초기화
```

로지스틱 회귀의 비용함수는 아래와 같다.

$$
J(\boldsymbol{\theta}) = -\dfrac{1}{m} \sum\limits_{i=1}^{m}{\left[ y^{(i)} log\left(\hat{p}^{(i)}\right) + (1 - y^{(i)}) log\left(1 - \hat{p}^{(i)}\right)\right]}
$$

배치 경사하강법 훈련은 아래 코드를 통해 이루어진다.

- `eta = 0.1`: 학습률
- `n_iterations = 5001` : 에포크 수
- `m = len(X_train)`: 훈련 세트 크기, 즉 훈련 샘플 수
- `epsilon = 1e-7`: $\log$ 값이 항상 계산되도록 더해지는 작은 실수
- `logits`: 모든 샘플에 대한 클래스별 점수, 즉 $\mathbf{X}_{\textit{train}}\, \Theta$
- `Y_proba`: 모든 샘플에 대해 계산된 클래스 별 소속 확률, 즉 $\hat P$


```python
#  배치 경사하강법 구현
eta = 0.1
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7

for iteration in range(n_iterations):     # 5001번 반복 훈련
    logits = X_train.dot(Theta)
    Y_proba = logistic(logits)
   
    if iteration % 500 == 0:
      loss = -np.mean(np.sum((y_train*np.log(Y_proba + epsilon) + (1-y_train)*np.log(1 - Y_proba + epsilon))))
      print(iteration, loss)
    
    error = Y_proba - y_train     # 그레이디언트 계산.
    gradients = 1/m * X_train.T.dot(error)
    
    Theta = Theta - eta * gradients
```

    0 79.35473984499612
    500 27.149524631560638
    1000 21.894389285779454
    1500 19.337773447717066
    2000 17.691444239326714
    2500 16.49516908325313
    3000 15.566000472955372
    3500 14.81327398979558
    4000 14.185530546071133
    4500 13.650751548055764
    5000 13.187653637231028
    


```python
Theta
```




    array([-10.56492618,   0.53611169,   4.82694082])



코드의 세부동작은 다음과 같다.

1. `logits = X_train.dot(Theta)` 에서 행렬연산을 이용해 세타와 x값을 곱하여 `logits` 값을 얻는다.
2. `Y_proba = logistic(logits)` 에 `logits`값을 시그모이드 함수에 넣어 `Y_proba`값을 얻는다.

3. 손실함수 계산을 통해 손실비용 `loss`를 얻는다.

4. y의 확률값과 실제 y값의 차이 `error`를 얻는다.

5. 이를 통해 `gradient` 계산을한다.

6. 세타에 `eta * gradient`만큼의 값을 빼서 세타값 재조정을 한뒤 다음 에포크로 넘어간다.


**검증**  
위에서 얻은 세타값을 이용하여 검증세트로 모델 성능을 판단한다.  
`Y_proba`값이 0.5 이상이라면 1로, 아니면 0으로 입력한다.  
(1이면 버지니카로 분류, 0이면 버지니카가 아닌 것으로 분류)


```python
logits = X_valid.dot(Theta)              
Y_proba = logistic(logits)
y_predict = np.array([])
for i in range(len(Y_proba)):
  if Y_proba[i] >= 0.5:
    y_predict = np.append(y_predict, 1)
  else:
    y_predict = np.append(y_predict, 0)

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```




    0.9666666666666667



**모델 규제**  
일반적으로 l2규제를 사용한다.
비용 함수는 아래와 같다.

$$
\begin{align*}
J(\boldsymbol{\theta}) & = \text{MSE}(\boldsymbol{\theta}) + \dfrac{\alpha}{2}\sum\limits_{i=1}^{n}{\theta_i}^2 \\
& = \text{MSE}(\boldsymbol{\theta}) + \dfrac{\alpha}{2}\left (\theta_1^2 + \cdots + \theta_n^2 \right )
\end{align*}
$$


```python
eta = 0.1
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.5        # 규제 하이퍼파라미터

Theta = np.random.randn(n_inputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    logits = X_train.dot(Theta)
    Y_proba = logistic(logits)
    
    if iteration % 500 == 0:
        xentropy_loss = -np.mean(np.sum((y_train*np.log(Y_proba + epsilon) + (1-y_train)*np.log(1 - Y_proba + epsilon))))
        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))  # 편향은 규제에서 제외
        loss = xentropy_loss + alpha * l2_loss        # l2 규제가 추가된 손실
        print(iteration, loss)
    
    error = Y_proba - y_train
    l2_loss_gradients = np.r_[np.zeros([1]), alpha * Theta[1:]]   # l2 규제 그레이디언트
    gradients = 1/m * X_train.T.dot(error) + l2_loss_gradients
    
    Theta = Theta - eta * gradients
```

    0 156.73838246234882
    500 36.11974638424874
    1000 34.306068180110614
    1500 34.02211206089248
    2000 33.9713877223945
    2500 33.96211929178583
    3000 33.96041878356459
    3500 33.960106551185575
    4000 33.96004921390298
    4500 33.96003868441417
    5000 33.96003675075696
    

검증 세트를 이용하여 성능을 판단한다.


```python
logits = X_valid.dot(Theta)              
Y_proba = logistic(logits)
y_predict = np.array([])
for i in range(len(Y_proba)):
  if Y_proba[i] >= 0.5:
    y_predict = np.append(y_predict, 1)
  else:
    y_predict = np.append(y_predict, 0)

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```




    0.8333333333333334



점수가 조금 낮아졌다.  
이번에는 **조기종료** 기능을 추가한다.  
조기종료는 검증세트에 대한 손실값이 이전 단계보다 커지면 바로 종료되는 기능이다.


```python
eta = 0.1 
n_iterations = 50000
m = len(X_train)
epsilon = 1e-7
alpha = 0.5            # 규제 하이퍼파라미터
best_loss = np.infty   # 최소 손실값 기억 변수

Theta = np.random.randn(n_inputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits = X_train.dot(Theta)
    Y_proba = logistic(logits)
    error = Y_proba - y_train
    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1]), alpha * Theta[1:]]
    Theta = Theta - eta * gradients

    # 검증 세트에 대한 손실 계산
    logits = X_valid.dot(Theta)
    Y_proba = logistic(logits)
    xentropy_loss = -np.mean(np.sum((y_valid*np.log(Y_proba + epsilon) + (1-y_valid)*np.log(1 - Y_proba + epsilon))))
    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))  # 편향은 규제를 하지 않음
    loss = xentropy_loss + alpha * l2_loss
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss)
        
    # 에포크마다 최소 손실값 업데이트
    if loss < best_loss:
        best_loss = loss
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss)        # 종료되기 이전 에포크의 손실값 출력
        print(iteration, loss, "조기 종료!")
        break
```

    0 31.861885822665705
    500 12.54052343500867
    1000 11.955093912404864
    1500 11.865469014733975
    2000 11.849523658203214
    2500 11.846612260301496
    3000 11.846078169747395
    3500 11.845980107187028
    4000 11.84596209939774
    4500 11.845958792428052
    5000 11.84595818512936
    5500 11.845958073603672
    6000 11.84595805312284
    6500 11.845958049361695
    7000 11.845958048670989
    7500 11.845958048544144
    8000 11.845958048520849
    8249 11.845958048517861
    8250 11.845958048517861 조기 종료!
    

테스트세트를 이용하여 정확도를 판별한다.


```python
logits = X_test.dot(Theta)
Y_proba = logistic(logits)
y_predict = np.array([])
for i in range(len(Y_proba)):
  if Y_proba[i] >= 0.5:
    y_predict = np.append(y_predict, 1)
  else:
    y_predict = np.append(y_predict, 0)


accuracy_score = np.mean(y_predict == y_test)
accuracy_score
```




    0.9666666666666667



정확도가 높게 측정되었다.  
사이킷런의 로지스틱 모델과 비교해보자.


```python
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(solver="lbfgs", random_state=42)
log_reg.fit(X_train, y_train)
```




    LogisticRegression(random_state=42)




```python
log_reg.score(X_test,y_test)
```




    0.9666666666666667



사이킷런의 로지스틱 모델과 유사한 성능임을 확인할 수 있다.

## 로지스틱 회귀로 다중 클래스 분류 알고리즘 구현 <br/> (역시 사이킷런을 사용하지 않음)

위에서 사이킷런없이 직접 구현한 로지스틱 모델에  
일대다(OvR)방식을 적용하여 붓꽃에 대한  
다중 클래스 분류 알고리즘을 구현할 것이다.  

위에서는 이진 분류로 버지니카인지 아닌지만 구별했다면  
이번에는 각각의 품종으로 분류한다.  

이를 수행하기 위해서 로지스틱 모델을 2개 사용해야 한다.  
setosa를 분류하는 모델, virginica를 분류하는 모델을 만들고,  
1 - (setosa 확률) - (virginica 확률)로 visicolor를 분류한다.

위에서처럼 데이터셋을 분할한다


```python
X = iris["data"][:, (2, 3)]  # 꽃잎 길이, 꽃잎 넓이
y = iris["target"]
y0 = (iris["target"] == 0).astype(np.int) #setosa 판단 모델을 위한 데이터셋
y1 = (iris["target"] == 2).astype(np.int) #virginica 판단 모델을 위한 데이터셋
```


```python
X_with_bias = np.c_[np.ones([len(X), 1]), X] #편향추가
```


```python
np.random.seed(2042) #일정한 결과를 위해 랜덤시드 지정
```


```python
test_ratio = 0.2                                         # 테스트 세트 비율 = 20%
validation_ratio = 0.2                                   # 검증 세트 비율 = 20%
total_size = len(X_with_bias)                            # 전체 데이터셋 크기

test_size = int(total_size * test_ratio)                 # 테스트 세트 크기: 전체의 20%
validation_size = int(total_size * validation_ratio)     # 검증 세트 크기: 전체의 20%
train_size = total_size - test_size - validation_size    # 훈련 세트 크기: 전체의 60%
```


```python
rnd_indices = np.random.permutation(total_size) #데이터 섞기
```

모델 훈련은 각 클래스에 대해 각각 이루어지기 때문에  
각각의 데이터셋을 개별적으로 준비한다.


```python
X_train = X_with_bias[rnd_indices[:train_size]] 
y_train = y[rnd_indices[:train_size]]
y_train0 = y0[rnd_indices[:train_size]] #setosa에 대한 라벨
y_train1 = y1[rnd_indices[:train_size]] #virginica에 대한 라벨

X_valid = X_with_bias[rnd_indices[train_size:-test_size]]
y_valid = y[rnd_indices[train_size:-test_size]]
y_valid0 = y0[rnd_indices[train_size:-test_size]] #setosa에 대한 검증세트 라벨
y_valid1 = y1[rnd_indices[train_size:-test_size]] #virginica에 대한 검증세트 라벨

X_test = X_with_bias[rnd_indices[-test_size:]]
y_test = y[rnd_indices[-test_size:]]
```


```python
n_inputs = X_train.shape[1]
Theta0 = np.random.randn(n_inputs) #setosa 판단모델에 쓰이는 세타값
Theta1 = np.random.randn(n_inputs) #virginica 판단모델에 쓰이는 세타값
```

**setosa 분류 로지스틱 회귀 알고리즘**


```python
eta = 0.1 
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.5            # 규제 하이퍼파라미터
best_loss0 = np.infty   # 최소 손실값 기억 변수

Theta0 = np.random.randn(n_inputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits0 = X_train.dot(Theta0)
    Y_proba0 = logistic(logits0)
    error = Y_proba0 - y_train0
    gradients0 = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1]), alpha * Theta0[1:]]
    Theta0 = Theta0 - eta * gradients0

    # 검증 세트에 대한 손실 계산
    logits0 = X_valid.dot(Theta0)
    Y_proba0 = logistic(logits0)
    xentropy_loss0 = -np.mean(np.sum((y_valid0*np.log(Y_proba0 + epsilon) + (1-y_valid0)*np.log(1 - Y_proba0 + epsilon))))
    l2_loss0 = 1/2 * np.sum(np.square(Theta0[1:]))
    loss0 = xentropy_loss0 + alpha * l2_loss0
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss0)
        
    # 에포크마다 최소 손실값 업데이트
    if loss0 < best_loss0:
        best_loss0 = loss0
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss0)        # 종료되기 이전 에포크의 손실값 출력
        print(iteration, loss0, "조기 종료!")
        break
```

    0 20.540019459712514
    500 7.7445716153439585
    1000 7.672989036271925
    1500 7.668592640555668
    2000 7.668314272027711
    2500 7.668296612120626
    3000 7.668295491624586
    3500 7.668295420530142
    4000 7.668295416019263
    4500 7.66829541573305
    5000 7.668295415714892
    

**virginica 분류 로지스틱 회귀 알고리즘**


```python
eta = 0.1 
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.5            # 규제 하이퍼파라미터
best_loss1 = np.infty   # 최소 손실값 기억 변수

Theta1 = np.random.randn(n_inputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits1 = X_train.dot(Theta1)
    Y_proba1 = logistic(logits1)
    error = Y_proba1 - y_train1
    gradients1 = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1]), alpha * Theta1[1:]]
    Theta1 = Theta1 - eta * gradients1

    # 검증 세트에 대한 손실 계산
    logits1 = X_valid.dot(Theta1)
    Y_proba1 = logistic(logits1)
    xentropy_loss1 = -np.mean(np.sum((y_valid1*np.log(Y_proba1 + epsilon) + (1-y_valid1)*np.log(1 - Y_proba1 + epsilon))))
    l2_loss1 = 1/2 * np.sum(np.square(Theta1[1:]))
    loss1 = xentropy_loss1 + alpha * l2_loss1
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss1)
        
    # 에포크마다 최소 손실값 업데이트
    if loss1 < best_loss1:
        best_loss1 = loss1
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss1)        # 종료되기 이전 에포크의 손실값 출력
        print(iteration, loss1, "조기 종료!")
        break
```

    0 45.3881848638996
    500 12.482904005693056
    1000 11.947222069327108
    1500 11.864096195806564
    2000 11.849273910674974
    2500 11.846566475123907
    3000 11.846069764314985
    3500 11.845978563684064
    4000 11.845961815948371
    4500 11.845958740374874
    5000 11.845958175570193
    

이제 테스트셋이 적용해보자.  
위에서 구한 두 개의 세타값을 이용한다.  

1. setosa일 확률 `setosa_proba`
2. virginica일 확률 `virginica_proba`
3. versicolor일 확률 `1 - setosa_proba - virginica_proba`  

3개 중 가장 높은 점수가 나온 클래스로 분류한다.  
(setosa일 확률이 가장 높으면 setosa로 분류)


```python
logits = X_test.dot(Theta0) #setosa에 대한 확률값 추정  
setosa_proba = logistic(logits)

logits = X_test.dot(Theta1) #virginica에 대한 확률값 추정 
virginica_proba = logistic(logits)

y_predict = np.array([])
for i in range(len(Y_proba0)):
  prob_list = [[setosa_proba[i], 0], [1-setosa_proba[i]-virginica_proba[i], 1], [virginica_proba[i], 2]]
  prob_list.sort(reverse=True) #가장 높은 확률이 가장 앞으로 오게끔 정렬해준다.
  y_predict = np.append(y_predict, prob_list[0][1]) #가장 확률이 높았던 것을 예측값으로 결정한다.
```


```python
accuracy_score = np.mean(y_predict == y_test)
accuracy_score
```




    0.9333333333333333



0.93으로 꽤 좋은 점수가 나왔다.  
이제 사이킷런의 로지스틱 모델과 비교해보자.  

`LogisticRegression`의 파라미터 `solver`의 값을 `newton-cg`로 하면  
로지스틱 회귀 모델이 다중 클래스 분류를 한다.


```python
from sklearn.linear_model import LogisticRegression
multi_log_reg = LogisticRegression(solver='newton-cg', random_state=42).fit(X_train,y_train)

multi_log_reg.score(X_test,y_test)
```




    0.9333333333333333



0.93으로 같은 점수가 나왔다.   
직접 구현한 로지스틱 알고리즘이  
사이킷런의 로지스틱 모델과 같은 성능임을 확인할 수 있다.
